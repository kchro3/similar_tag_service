{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6daa4e-a2e3-46fd-8a1a-24b150e48344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afeb97e3-eed3-400f-9b9a-99e69a569991",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [\n",
    "    stem_text,\n",
    "    strip_tags,\n",
    "    strip_punctuation,\n",
    "    strip_multiple_whitespaces,\n",
    "    remove_stopwords,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6288221-c36a-49d3-b860-27cbca04a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "post_df = pd.read_json(\"data/preprocessed_data.jsonl\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8b311a3-a35e-4dfb-b899-9b12017239d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jolyne cujoh',\n",
       " 'giorno giovanna',\n",
       " 'josuke higashikata',\n",
       " 'kitten sized',\n",
       " 'myth',\n",
       " 'fengqing',\n",
       " 'mu qing',\n",
       " 'feng xin',\n",
       " 'ðŸ¥° just a little boy ðŸš¯ i will kiss ðŸ’‹ your heart',\n",
       " 'so true']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = []\n",
    "for tags in post_df.head().tags.to_list():\n",
    "    flattened += tags\n",
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99ff7f34-00f4-42a5-a670-0581533c676d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('jolyn cujoh', ['jolyne cujoh']),\n",
       "             ('giorno giovanna', ['giorno giovanna']),\n",
       "             ('josuk higashikata', ['josuke higashikata']),\n",
       "             ('kitten size', ['kitten sized']),\n",
       "             ('myth', ['myth']),\n",
       "             ('fengq', ['fengqing']),\n",
       "             ('mu qing', ['mu qing']),\n",
       "             ('feng xin', ['feng xin']),\n",
       "             ('ðŸ¥° littl boi ðŸš¯ kiss ðŸ’‹ heart',\n",
       "              ['ðŸ¥° just a little boy ðŸš¯ i will kiss ðŸ’‹ your heart']),\n",
       "             ('true', ['so true'])])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "canonical = OrderedDict()\n",
    "for word in flattened:\n",
    "    p_word = ' '.join(preprocess_string(word, FILTERS))\n",
    "    if p_word not in canonical:\n",
    "        canonical[p_word] = [word]\n",
    "    else:\n",
    "        canonical[p_word].append(word)\n",
    "canonical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80d18680-ccc5-4f2f-a07e-9c99a19858fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "1.5\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for x in np.mean(np.array([[1,2,3],[2,1,3]]), axis=0):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ca5ce1b-aa12-4c09-8e83-c7a577b4b8d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m train \u001b[38;5;241m=\u001b[39m HFTrainer()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlanguage-modeling\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/txtai/pipeline/train/hftrainer.py:45\u001b[0m, in \u001b[0;36mHFTrainer.__call__\u001b[0;34m(self, base, train, validation, columns, maxlength, stride, task, prefix, metrics, **args)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mBuilds a new model using arguments.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    (model, tokenizer)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Parse TrainingArguments\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Set seed for model reproducibility\u001b[39;00m\n\u001b[1;32m     48\u001b[0m set_seed(args\u001b[38;5;241m.\u001b[39mseed)\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/txtai/pipeline/train/hftrainer.py:119\u001b[0m, in \u001b[0;36mHFTrainer.parse\u001b[0;34m(self, updates)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Apply custom arguments\u001b[39;00m\n\u001b[1;32m    117\u001b[0m args\u001b[38;5;241m.\u001b[39mupdate(updates)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:108\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode)\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/transformers/training_args.py:1176\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim \u001b[38;5;241m=\u001b[39m OptimizerNames\u001b[38;5;241m.\u001b[39mADAFACTOR\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1175\u001b[0m ):\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1179\u001b[0m     )\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1189\u001b[0m ):\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--bf16_full_eval`) can only be used on CUDA or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1193\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertConfig, BertForMaskedLM\n",
    "\n",
    "from txtai.pipeline import HFTrainer\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size = 500,\n",
    "    hidden_size = 50,\n",
    "    num_hidden_layers = 2,\n",
    "    num_attention_heads = 2,\n",
    "    intermediate_size = 100,\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "model.save_pretrained(\"bert\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert\")\n",
    "\n",
    "train = HFTrainer()\n",
    "\n",
    "# Train model\n",
    "train((model, tokenizer), dataset, task=\"language-modeling\", output_dir=\"bert\",\n",
    "      fp16=True, per_device_train_batch_size=128, num_train_epochs=10,\n",
    "      dataloader_num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cddf4f-470a-4963-b568-1fb210966422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
