{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a800228-772e-47e0-9272-a1413d217dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffhara/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from txtai.embeddings import Embeddings\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1941d20-9542-4472-9434-7a75fe739766",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings({\"path\": \"sentence-transformers/all-mpnet-base-v2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf94b18a-e3df-4e78-bc14-9b7fbac1262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "post_df = pd.read_json(\"data/preprocessed_data.jsonl\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e5a873-86d9-4002-9af2-7d2144dbb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1caa7cf-4f55-4d9f-80f0-b519f15ba838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278808"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tags = []\n",
    "for i, row in post_df.iterrows():\n",
    "    for tag in row[\"tags\"] + row[\"root_tags\"]:\n",
    "        tags.append(tag.lower())\n",
    "\n",
    "documents = []\n",
    "tag_by_id = {}\n",
    "for i, (tag, count) in enumerate(Counter(tags).most_common()):\n",
    "    documents.append((i, tag, None))\n",
    "    tag_by_id[i] = tag\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f9bec59-5f35-4b8b-9df0-bd091dc8f6df",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/txtai/embeddings/base.py:115\u001b[0m, in \u001b[0;36mEmbeddings.index\u001b[0;34m(self, documents, reindex)\u001b[0m\n\u001b[1;32m    111\u001b[0m transform \u001b[38;5;241m=\u001b[39m Transform(\u001b[38;5;28mself\u001b[39m, Action\u001b[38;5;241m.\u001b[39mREINDEX \u001b[38;5;28;01mif\u001b[39;00m reindex \u001b[38;5;28;01melse\u001b[39;00m Action\u001b[38;5;241m.\u001b[39mINDEX)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m buffer:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Load documents into database and transform to vectors\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     ids, dimensions, embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids:\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;66;03m# Build LSA model (if enabled). Remove principal components from embeddings.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/txtai/embeddings/transform.py:58\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, documents, buffer)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mProcesses an iterable collection of documents, handles any iterable including generators.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    (document ids, dimensions, embeddings)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Transform documents to vectors and load into database\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m ids, dimensions, batches, stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Check that embeddings are available and load as a memmap\u001b[39;00m\n\u001b[1;32m     61\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/txtai/vectors/base.py:86\u001b[0m, in \u001b[0;36mVectors.index\u001b[0;34m(self, documents, batchsize)\u001b[0m\n\u001b[1;32m     82\u001b[0m batch\u001b[38;5;241m.\u001b[39mappend(document)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m batchsize:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Convert batch to embeddings\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     uids, dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     ids\u001b[38;5;241m.\u001b[39mextend(uids)\n\u001b[1;32m     88\u001b[0m     batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/txtai/vectors/base.py:152\u001b[0m, in \u001b[0;36mVectors.batch\u001b[0;34m(self, documents, output)\u001b[0m\n\u001b[1;32m    149\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Build embeddings\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     dimensions \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/txtai/vectors/transformers.py:48\u001b[0m, in \u001b[0;36mTransformersVectors.encode\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Encode data using vectors model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencodebatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/txtai/models/pooling.py:75\u001b[0m, in \u001b[0;36mPooling.encode\u001b[0;34m(self, documents, batch)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Run inputs through model\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 75\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Add batch result\u001b[39;00m\n\u001b[1;32m     78\u001b[0m results\u001b[38;5;241m.\u001b[39mextend(outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/txtai/models/pooling.py:129\u001b[0m, in \u001b[0;36mMeanPooling.forward\u001b[0;34m(self, **inputs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03mRuns mean pooling on token embeddings taking the input mask into account.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    mean pooled embeddings using output token embeddings (i.e. last hidden state)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Run through transformers model\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Mean pooling\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# pylint: disable=E1101\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/txtai/models/pooling.py:108\u001b[0m, in \u001b[0;36mPooling.forward\u001b[0;34m(self, **inputs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    Runs inputs through transformers model and returns outputs.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m        model outputs\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1019\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1010\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1012\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1013\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1014\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1018\u001b[0m )\n\u001b[0;32m-> 1019\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1032\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:609\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    600\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    601\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    602\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/transformers/pytorch_utils.py:249\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:462\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 462\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/similar_tags_service/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings.index(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54740bdd-4999-4596-bfb9-894fefbdece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: tuserkaty\n",
      "tuserkay 0.8877871632575989\n",
      "tusermelissa 0.804341733455658\n",
      "tuseral 0.7983123660087585\n",
      "tusernicky 0.7970232963562012\n",
      "tusercat 0.790561854839325\n",
      "tuserdi 0.787990927696228\n",
      "tusergeo 0.7879773378372192\n",
      "tusereve 0.7808942794799805\n",
      "tuserabbie 0.7787960767745972\n",
      "tusershay 0.7774391174316406\n",
      "\n",
      "seed: not touhou\n",
      "touhou 0.858487606048584\n",
      "touhou project 0.7077557444572449\n",
      "tokusatsu 0.6249366998672485\n",
      "tokrev 0.5637601017951965\n",
      "touya todoroki 0.5554528832435608\n",
      "toa 0.53392094373703\n",
      "todoroki touya 0.5284604430198669\n",
      "toh king 0.5228137969970703\n",
      "hitori gotou 0.520067572593689\n",
      "\n",
      "seed: rusame exchange\n",
      "\n",
      "seed: meat painting\n",
      "painting 0.6750514507293701\n",
      "meat 0.6640787720680237\n",
      "animal art 0.6308740377426147\n",
      "paint 0.6155071258544922\n",
      "paintings 0.6053147315979004\n",
      "oil painting 0.5972870588302612\n",
      "painter 0.5898380875587463\n",
      "digital painting 0.5874813795089722\n",
      "animal illustration 0.5513209104537964\n",
      "food photography 0.5461929440498352\n",
      "\n",
      "seed: i love this music video\n",
      "music video 0.706646203994751\n",
      "funny video 0.5532892942428589\n",
      "i love this so much 0.5448477864265442\n",
      "i love this 0.5115219354629517\n",
      "song 0.5079655647277832\n",
      "i love it 0.5075878500938416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tag_df = pd.read_csv(\"data/Trial-supplement-tag-followers.csv\")\n",
    "\n",
    "for i, row in tag_df.tail(1000).sample(5).iterrows():\n",
    "    print('seed:', row['tag'])\n",
    "    for doc, score in embeddings.search(row['tag'], 10):\n",
    "        if 0.5 < score < 0.9:\n",
    "            print(tag_by_id[doc], score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce035b4e-5738-4ad3-a547-5cb20ab78c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blog_url</th>\n",
       "      <th>doc</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100535</th>\n",
       "      <td>indecisivecaffeineaddict.tumblr.com</td>\n",
       "      <td>Me mutuals logging Tumblr daily harvest hoes b...</td>\n",
       "      <td>[lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45594</th>\n",
       "      <td>magnvmchasma.tumblr.com</td>\n",
       "      <td>meredithhunnter blog Gustav Heurlin aka Gustav...</td>\n",
       "      <td>[holy fire, cash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30775</th>\n",
       "      <td>small-giant-plans.tumblr.com</td>\n",
       "      <td>books sense ve read tbh nona the ninth gideon ...</td>\n",
       "      <td>[locked tomb, nona the ninth spoilers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68474</th>\n",
       "      <td>colourthewavesandwalls.tumblr.com</td>\n",
       "      <td>fluffygif Magical moment captured nature wild ...</td>\n",
       "      <td>[animals, nature, you queue here?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64511</th>\n",
       "      <td>traditionalfly.tumblr.com</td>\n",
       "      <td>Gossips Vadim Gorodnitsky birds nature photogr...</td>\n",
       "      <td>[photography, birds, animals]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66563</th>\n",
       "      <td>dabbingonronpa.tumblr.com</td>\n",
       "      <td>One Prime transformers prime tfp transformers ...</td>\n",
       "      <td>[JUNE DARBY XD XD, go QUEEN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>umidays.tumblr.com</td>\n",
       "      <td>soft babies 🤍 they’re everything to me 🥺 akian...</td>\n",
       "      <td>[chainsaw man, fanart]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82956</th>\n",
       "      <td>simstoricalishccfinds.tumblr.com</td>\n",
       "      <td>Simblreen 2020 Treat 1The Sophia 1824 Evening ...</td>\n",
       "      <td>[ts4 regency]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6452</th>\n",
       "      <td>karmahope.tumblr.com</td>\n",
       "      <td>lowkey haikyuu phase haikyuu haikyuu textposts...</td>\n",
       "      <td>[hq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66550</th>\n",
       "      <td>enniewritesathing.tumblr.com</td>\n",
       "      <td>unbelievably funny melook pleased ts3 oc: lady...</td>\n",
       "      <td>[tzr, you think the taxi driver gave her treat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   blog_url  \\\n",
       "100535  indecisivecaffeineaddict.tumblr.com   \n",
       "45594               magnvmchasma.tumblr.com   \n",
       "30775          small-giant-plans.tumblr.com   \n",
       "68474     colourthewavesandwalls.tumblr.com   \n",
       "64511             traditionalfly.tumblr.com   \n",
       "...                                     ...   \n",
       "66563             dabbingonronpa.tumblr.com   \n",
       "5406                     umidays.tumblr.com   \n",
       "82956      simstoricalishccfinds.tumblr.com   \n",
       "6452                   karmahope.tumblr.com   \n",
       "66550          enniewritesathing.tumblr.com   \n",
       "\n",
       "                                                      doc  \\\n",
       "100535  Me mutuals logging Tumblr daily harvest hoes b...   \n",
       "45594   meredithhunnter blog Gustav Heurlin aka Gustav...   \n",
       "30775   books sense ve read tbh nona the ninth gideon ...   \n",
       "68474   fluffygif Magical moment captured nature wild ...   \n",
       "64511   Gossips Vadim Gorodnitsky birds nature photogr...   \n",
       "...                                                   ...   \n",
       "66563   One Prime transformers prime tfp transformers ...   \n",
       "5406    soft babies 🤍 they’re everything to me 🥺 akian...   \n",
       "82956   Simblreen 2020 Treat 1The Sophia 1824 Evening ...   \n",
       "6452    lowkey haikyuu phase haikyuu haikyuu textposts...   \n",
       "66550   unbelievably funny melook pleased ts3 oc: lady...   \n",
       "\n",
       "                                                     tags  \n",
       "100535                                              [lol]  \n",
       "45594                                   [holy fire, cash]  \n",
       "30775              [locked tomb, nona the ninth spoilers]  \n",
       "68474                  [animals, nature, you queue here?]  \n",
       "64511                       [photography, birds, animals]  \n",
       "...                                                   ...  \n",
       "66563                        [JUNE DARBY XD XD, go QUEEN]  \n",
       "5406                               [chainsaw man, fanart]  \n",
       "82956                                       [ts4 regency]  \n",
       "6452                                                 [hq]  \n",
       "66550   [tzr, you think the taxi driver gave her treat...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import *\n",
    "\n",
    "def strip_newlines(x):\n",
    "    return x.replace('\\\\n', ' ')\n",
    "\n",
    "filters = [\n",
    "    strip_newlines,\n",
    "    strip_tags,\n",
    "    strip_punctuation,\n",
    "    strip_multiple_whitespaces,\n",
    "    remove_stopwords,\n",
    "]\n",
    "\n",
    "df = post_df.sample(100)\n",
    "df['bodyp'] = df.body.map(lambda x: ' '.join(preprocess_string(x, filters)))\n",
    "df['titlep'] = df.title.map(lambda x: preprocess_string(x, filters))\n",
    "df['doc'] = list(map(lambda x: ' '.join(x[0] + x[1]), zip(df.titlep, df.root_tags)))\n",
    "df[['blog_url', 'doc', 'tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda50500-29f8-4456-aaf3-2a13c3eca1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = []\n",
    "docs_by_post = {}\n",
    "tags_by_post = {}\n",
    "for i, row in df.iterrows():\n",
    "    documents.append((i, row['doc'][:100], None))\n",
    "    tags_by_post[i] = row['tags']\n",
    "    docs_by_post[i] = row['doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016eafd7-381d-4542-8b27-fa1f3367a7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████▋                                  | 52/105 [30:50<30:44, 34.81s/it]"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    embeddings.upsert(documents[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "68da3a7f-0130-4346-bc71-c23e1b06c9c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Srvlst\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(67308, Srvlst, None)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(37643, Srvlst, None)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(102622, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(82331, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(77985, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(18129, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(13605, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(13403, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(46578, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(43145, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(42438, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(41361, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(39315, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(38177, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(37857, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(37478, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(35773, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(22898, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(22481, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(20550, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(20304, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(17013, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(15853, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(14028, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(12227, sr, None)</td>\n",
       "      <td>0.666234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      tag     score\n",
       "0   (67308, Srvlst, None)  1.000000\n",
       "1   (37643, Srvlst, None)  1.000000\n",
       "2      (102622, sr, None)  0.666234\n",
       "3       (82331, sr, None)  0.666234\n",
       "4       (77985, sr, None)  0.666234\n",
       "5       (18129, sr, None)  0.666234\n",
       "6       (13605, sr, None)  0.666234\n",
       "7       (13403, sr, None)  0.666234\n",
       "8       (46578, sr, None)  0.666234\n",
       "9       (43145, sr, None)  0.666234\n",
       "10      (42438, sr, None)  0.666234\n",
       "11      (41361, sr, None)  0.666234\n",
       "12      (39315, sr, None)  0.666234\n",
       "13      (38177, sr, None)  0.666234\n",
       "14      (37857, sr, None)  0.666234\n",
       "15      (37478, sr, None)  0.666234\n",
       "16      (35773, sr, None)  0.666234\n",
       "17      (22898, sr, None)  0.666234\n",
       "18      (22481, sr, None)  0.666234\n",
       "19      (20550, sr, None)  0.666234\n",
       "20      (20304, sr, None)  0.666234\n",
       "21      (17013, sr, None)  0.666234\n",
       "22      (15853, sr, None)  0.666234\n",
       "23      (14028, sr, None)  0.666234\n",
       "24      (12227, sr, None)  0.666234"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def printdf(post_df, tag):\n",
    "    rows = []\n",
    "    for doc, score in embeddings.search(tag, 25):\n",
    "        rows.append({\n",
    "            \"tag\": documents[doc],\n",
    "            \"score\": score,\n",
    "        })\n",
    "    display(pd.DataFrame(rows))\n",
    "\n",
    "for i, row in post_df.sample(1).iterrows():\n",
    "    for tag in row['tags']:\n",
    "        print(tag)\n",
    "        printdf(post_df, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6c5ca3ad-a0b0-4e5e-bafd-56d3eda0eb29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video games\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>tag</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Youth fascination technology They love phone g...</td>\n",
       "      <td>[q]</td>\n",
       "      <td>0.488692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boozerman DRAGON AGE INQUISITION motionresque ...</td>\n",
       "      <td>[dragon age]</td>\n",
       "      <td>0.465870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SILT ∙ Spiral Circus 2022 silt gamingedit indi...</td>\n",
       "      <td>[rbs, games, silt]</td>\n",
       "      <td>0.441177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slushiecafe bark bark plays owl city idog nost...</td>\n",
       "      <td>[idog, dogs, robots]</td>\n",
       "      <td>0.436060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bamf Cyberpunk 2077 ↪ RIDERS ON THE STORM cp20...</td>\n",
       "      <td>[wife, cyberpunk 2077, panam palmer, panam my ...</td>\n",
       "      <td>0.413248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc  \\\n",
       "0  Youth fascination technology They love phone g...   \n",
       "1  boozerman DRAGON AGE INQUISITION motionresque ...   \n",
       "2  SILT ∙ Spiral Circus 2022 silt gamingedit indi...   \n",
       "3  slushiecafe bark bark plays owl city idog nost...   \n",
       "4  bamf Cyberpunk 2077 ↪ RIDERS ON THE STORM cp20...   \n",
       "\n",
       "                                                 tag     score  \n",
       "0                                                [q]  0.488692  \n",
       "1                                       [dragon age]  0.465870  \n",
       "2                                 [rbs, games, silt]  0.441177  \n",
       "3                               [idog, dogs, robots]  0.436060  \n",
       "4  [wife, cyberpunk 2077, panam palmer, panam my ...  0.413248  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genshin impact\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>tag</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recent genshin impact tighnari genshin tighnari</td>\n",
       "      <td>[shroom tigh, tighnari]</td>\n",
       "      <td>0.738725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My bro got traumatized genshin childe genshin ...</td>\n",
       "      <td>[reblogs, text, Queue will have order]</td>\n",
       "      <td>0.648070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAPPY BIRTHDAY TO YOU Wanderer Scaramouche Gen...</td>\n",
       "      <td>[other people's art, genshin impact, scaramouc...</td>\n",
       "      <td>0.535647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nation divided genshin impact genshin genshin ...</td>\n",
       "      <td>[genshin impact, raiden ei, art]</td>\n",
       "      <td>0.518707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cfeather light sea my art genshin impact xiao ...</td>\n",
       "      <td>[never played genshin but this is gorgeous, ar...</td>\n",
       "      <td>0.500026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc  \\\n",
       "0    recent genshin impact tighnari genshin tighnari   \n",
       "1  My bro got traumatized genshin childe genshin ...   \n",
       "2  HAPPY BIRTHDAY TO YOU Wanderer Scaramouche Gen...   \n",
       "3  nation divided genshin impact genshin genshin ...   \n",
       "4  cfeather light sea my art genshin impact xiao ...   \n",
       "\n",
       "                                                 tag     score  \n",
       "0                            [shroom tigh, tighnari]  0.738725  \n",
       "1             [reblogs, text, Queue will have order]  0.648070  \n",
       "2  [other people's art, genshin impact, scaramouc...  0.535647  \n",
       "3                   [genshin impact, raiden ei, art]  0.518707  \n",
       "4  [never played genshin but this is gorgeous, ar...  0.500026  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alhaitham\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>tag</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Congratulations Al Haitham relatable character...</td>\n",
       "      <td>[genshin impact]</td>\n",
       "      <td>0.532969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I’m process writing “Alhaitham propaganda” pos...</td>\n",
       "      <td>[alhaitham, genshin impact, breaking news: the...</td>\n",
       "      <td>0.494797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This I imagine Alhaitham messes Kaveh Addition...</td>\n",
       "      <td>[alhaitham, kaveh, art, genshin, kavetham]</td>\n",
       "      <td>0.485261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اسلام آباد ڈسٹرکٹ بار ایسوسی ایشن کے انتخابی ن...</td>\n",
       "      <td>[breaking news, pakistan, latest news in pakis...</td>\n",
       "      <td>0.444291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Al Haitham The Fattening Knowledge belly kink ...</td>\n",
       "      <td>[inflation art]</td>\n",
       "      <td>0.416389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc  \\\n",
       "0  Congratulations Al Haitham relatable character...   \n",
       "1  I’m process writing “Alhaitham propaganda” pos...   \n",
       "2  This I imagine Alhaitham messes Kaveh Addition...   \n",
       "3  اسلام آباد ڈسٹرکٹ بار ایسوسی ایشن کے انتخابی ن...   \n",
       "4  Al Haitham The Fattening Knowledge belly kink ...   \n",
       "\n",
       "                                                 tag     score  \n",
       "0                                   [genshin impact]  0.532969  \n",
       "1  [alhaitham, genshin impact, breaking news: the...  0.494797  \n",
       "2         [alhaitham, kaveh, art, genshin, kavetham]  0.485261  \n",
       "3  [breaking news, pakistan, latest news in pakis...  0.444291  \n",
       "4                                    [inflation art]  0.416389  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def printdf(post_df, tag):\n",
    "    rows = []\n",
    "    for doc, score in embeddings.search(tag, 5):\n",
    "        rows.append({\n",
    "            \"doc\": docs_by_post[doc],\n",
    "            \"tag\": tags_by_post[doc],\n",
    "            \"score\": score,\n",
    "        })\n",
    "    display(pd.DataFrame(rows))\n",
    "\n",
    "for i, row in post_df.sample(1).iterrows():\n",
    "    for tag in row['tags']:\n",
    "        print(tag)\n",
    "        printdf(post_df, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e9ea3656-dd9a-44d2-b663-3c24cae0fc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104246"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cca075-cc8a-4c0b-b155-e7b34c7579f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
